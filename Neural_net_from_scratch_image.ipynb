{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stocastic gradient descent on Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Libraries\n",
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    Extracting training and testing data\n",
    "    '''\n",
    "    train_data = pd.read_csv(r'L:\\Starting Neural Network\\MNIST classification\\mnist_train.csv', header=None)\n",
    "    test_data = pd.read_csv(r'L:\\Starting Neural Network\\MNIST classification\\mnist_test.csv', header=None)\n",
    "    train_data1=np.array(train_data)\n",
    "    test_data1=np.array(test_data)\n",
    "    ''' training data '''\n",
    "    train_image=train_data1[:,1:]\n",
    "    train_label=train_data1[:,0].reshape(len(train_data1),1)\n",
    "    ''' testing data '''\n",
    "    test_image=test_data1[:,1:]\n",
    "    test_label=test_data1[:,0].reshape(len(test_data1),1)\n",
    "    ''' HOT-ENCODING training and testing labels'''\n",
    "    train_label_updated=hot_encode(train_label,10)\n",
    "    test_label_updated=hot_encode(test_label,10)\n",
    "    \n",
    "    ''' Normalizing image pixels '''\n",
    "    train_image=train_image/255\n",
    "    ''' returning '''\n",
    "    return train_image, train_label_updated, test_image, test_label_updated\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(label,limit):\n",
    "    '''\n",
    "    Function to hot encode the labels in the given limit\n",
    "    '''\n",
    "    one_hot_label=np.zeros([len(label),limit])\n",
    "    j=0\n",
    "    for i in label:\n",
    "        one_hot_label[j,i]=1\n",
    "        j=j+1\n",
    "    return one_hot_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self,train_data):\n",
    "        # Seed the random number generator\n",
    "        np.random.seed(1)\n",
    "        l1_size=np.shape(train_data)[1]\n",
    "        hidden_l1 = 300\n",
    "        hidden_l2 = 100\n",
    "        n_class = 10\n",
    "        stddev1 = np.sqrt(2/(l1_size+hidden_l1))\n",
    "        stddev2 = np.sqrt(2/(hidden_l1+hidden_l2))\n",
    "        stddev3 = np.sqrt(2/(hidden_l2+n_class))\n",
    "        # Initialize weghts and biases\n",
    "        self.synaptic_weights1 = np.random.normal(0,stddev1,[l1_size,hidden_l1])\n",
    "        self.synaptic_weights2 = np.random.normal(0,stddev2,[hidden_l1,hidden_l2])\n",
    "        self.synaptic_weights3 = np.random.normal(0,stddev3,[hidden_l2,n_class])\n",
    "        self.bias1 = 1\n",
    "        self.bias2 = 1\n",
    "        self.bias3 = 1\n",
    "        self.grad1=0\n",
    "        self.grad2=0\n",
    "        self.grad3=0\n",
    "        self.bias_grad1=0\n",
    "        self.bias_grad2=0\n",
    "        self.bias_grad3=0\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Takes in weighted sum of the inputs and normalizes\n",
    "        them through between 0 and 1 through a sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        The derivative of the sigmoid function used to\n",
    "        calculate necessary weight adjustments\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def train(self, training_inputs, training_outputs, training_iterations, mode=\"normal\"):\n",
    "        \"\"\"\n",
    "        We train the model through trial and error, adjusting the\n",
    "        synaptic weights each time to get a better result\n",
    "        \"\"\"\n",
    "        #J=[0]*training_iterations\n",
    "        drop_prob=0.7\n",
    "        learning_rate=0.01\n",
    "        for iteration in range(training_iterations):\n",
    "            # Pass training set through the neural network\n",
    "            inputs = training_inputs.astype(float).reshape(1,np.shape(train_data)[1])\n",
    "            #print (np.shape(inputs))\n",
    "            a1 = self.sigmoid(np.dot(inputs, self.synaptic_weights1)+self.bias1)\n",
    "            #a1 = self.dropout(a1,drop_prob)\n",
    "            #print (np.shape(a1))\n",
    "            a2 = self.sigmoid(np.dot(a1, self.synaptic_weights2)+self.bias2)\n",
    "            #a2 = self.dropout(a2,drop_prob)\n",
    "            #print (np.shape(a2))\n",
    "            #print(output)\n",
    "            \n",
    "            a3 = self.sigmoid(np.dot(a2, self.synaptic_weights3)+self.bias3)\n",
    "            print\n",
    "            # Multiply error by input and gradient of the sigmoid function\n",
    "            # Less confident weights are adjusted more through the nature of the function\n",
    "            if mode==\"normal\":\n",
    "                # Calculate the error rate\n",
    "                error = (training_outputs - a3)\n",
    "                #J[iteration]=np.sum(error,axis=0)\n",
    "                J=np.sum(error,axis=0)\n",
    "                #print(J)\n",
    "                '''weight gradient'''\n",
    "                self.grad3 = np.matmul(a2.T, error * self.sigmoid_derivative(a3))\n",
    "                self.grad2 = np.matmul(np.transpose(a1),np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2))\n",
    "                upd=np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                self.grad1 = np.matmul(np.transpose(inputs),(self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2))))\n",
    "                '''bias gradient'''\n",
    "                bias_grad3_tot = error * self.sigmoid_derivative(a3)\n",
    "                bias_grad2_tot = np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                upd1=np.matmul(error * self.sigmoid_derivative(a3),np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                bias_grad1_tot = (self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2)))\n",
    "                self.bias_grad3 = np.mean(bias_grad3_tot,axis=0)\n",
    "                self.bias_grad2 = np.mean(bias_grad2_tot)\n",
    "                self.bias_grad1 = np.mean(bias_grad1_tot)\n",
    "                #print (np.shape(bias_grad2_tot),self.bias_grad2,bias_grad2_tot)\n",
    "            \n",
    "            else:\n",
    "                cost = (training_outputs*np.log(a3)) + ((1-training_outputs)*np.log(1-a3))\n",
    "                #J[iteration]=np.sum(cost,axis=0)\n",
    "                J=np.sum(cost,axis=0)\n",
    "                #print(J)\n",
    "                error=a3-training_outputs\n",
    "                self.grad3 = np.matmul(a2.T, error)\n",
    "                self.grad2 = np.matmul(np.transpose(a1),np.matmul(error,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2))\n",
    "                upd=np.matmul(error,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                self.grad1 = np.matmul(np.transpose(inputs),(self.sigmoid_derivative(a1)*np.dot(upd,np.transpose(self.synaptic_weights2))))\n",
    "                '''bias gradient'''\n",
    "                bias_grad3_tot = error\n",
    "                bias_grad2_tot = np.matmul(error ,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                upd1=np.matmul(error ,np.transpose(self.synaptic_weights3))*self.sigmoid_derivative(a2)\n",
    "                bias_grad1_tot = (self.sigmoid_derivative(a1)*np.matmul(upd,np.transpose(self.synaptic_weights2)))\n",
    "                self.bias_grad3 = np.mean(bias_grad3_tot)\n",
    "                self.bias_grad2 = np.mean(bias_grad2_tot)\n",
    "                self.bias_grad1 = np.mean(bias_grad1_tot)\n",
    "            \n",
    "            # Adjust synaptic weights\n",
    "            self.synaptic_weights1 -= self.grad1*learning_rate\n",
    "            self.synaptic_weights2 -= self.grad2*learning_rate\n",
    "            self.synaptic_weights3 -= self.grad3*learning_rate\n",
    "            # Adjust bias\n",
    "            self.bias1 -= self.bias_grad1*learning_rate\n",
    "            self.bias2 -= self.bias_grad2*learning_rate\n",
    "            self.bias3 -= self.bias_grad3*learning_rate\n",
    "            \n",
    "        return J\n",
    "\n",
    "    def think(self, inputs):\n",
    "        \"\"\"\n",
    "        Pass inputs through the neural network to get output\n",
    "        \"\"\"\n",
    "        \n",
    "        inputs = inputs.astype(float)\n",
    "        a1 = self.sigmoid(np.dot(inputs, self.synaptic_weights1)+self.bias1)\n",
    "        a2 = self.sigmoid(np.dot(a1, self.synaptic_weights2)+self.bias2)\n",
    "        a3 = self.sigmoid(np.dot(a2, self.synaptic_weights3)+self.bias3)\n",
    "        return (a3)  \n",
    "    \n",
    "    def dropout(self, X, drop_probability):\n",
    "        keep_probability = 1 - drop_probability\n",
    "        mask = np.random.uniform(0, 1.0, X.shape) < keep_probability\n",
    "        #############################\n",
    "        #  Avoid division by 0 when scaling\n",
    "        #############################\n",
    "        if keep_probability > 0.0:\n",
    "            scale = (1/keep_probability)\n",
    "        else:\n",
    "            scale = 0.0\n",
    "        return mask * X * scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in iteration 0 : 0.09045062636856138\n",
      "Loss in iteration 1 : 0.04145274174505751\n",
      "Loss in iteration 2 : 0.03160731663888193\n",
      "Loss in iteration 3 : 0.025547476960524385\n",
      "Loss in iteration 4 : 0.0212593161248778\n",
      "Loss in iteration 5 : 0.018020108707408798\n",
      "Loss in iteration 6 : 0.015471121178215638\n",
      "Loss in iteration 7 : 0.01340315062359934\n",
      "Loss in iteration 8 : 0.011681672748561677\n",
      "Loss in iteration 9 : 0.010217771803327624\n",
      "Loss in iteration 10 : 0.008954499795339192\n",
      "Loss in iteration 11 : 0.007852827178249967\n",
      "Loss in iteration 12 : 0.0068824162796030505\n",
      "Loss in iteration 13 : 0.0060221228694104555\n",
      "Loss in iteration 14 : 0.005258108093038985\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    Getting data\n",
    "    '''\n",
    "    train_data,train_label,test_data,test_label = load_data()\n",
    "    iterations=15\n",
    "    #print(train_data)\n",
    "    hi=NeuralNetwork(train_data)\n",
    "    cost=[0]*iterations\n",
    "    for j in range (iterations):\n",
    "        cost_per_data=0\n",
    "        for i in range (len(train_data)):\n",
    "            cost_per_data += hi.train(train_data[i],train_label[i],1,\"log\")\n",
    "        cost[j] = abs(np.mean(cost_per_data)/len(train_data))\n",
    "        print(\"Loss in iteration\",j,\":\",cost[j])\n",
    "    plt.plot(cost)\n",
    "    plt.xlabel('Iterations', fontsize=16)\n",
    "    plt.ylabel('Cost', fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy: 95.91\n"
     ]
    }
   ],
   "source": [
    "var=0\n",
    "for check_it in range (len(test_image)):\n",
    "    ypred = np.argmax(hi.think(test_image[check_it]))\n",
    "    y = np.argmax(test_label_updated[check_it],axis=0)\n",
    "    if ypred == y:\n",
    "        var +=1\n",
    "accuracy = 100*(var/len(test_image))\n",
    "print(\"Testing accuracy:\",accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code provided with testing accurac of 94.74% on training using 5 iterations\n",
    "The code provided with testing accurac of 95.91% on training using 15 iterations\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
